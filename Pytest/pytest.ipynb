{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bcc563d",
   "metadata": {},
   "source": [
    "# PYTEST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662b980f",
   "metadata": {},
   "source": [
    "Tutorial: https://tutorial.edu.lat/pub/pytest?alias=tutorial-pytest\n",
    "\n",
    "Guida Rapida: https://tutorial.edu.lat/pub/pytest/pytest-quick-guide/pytest-guida-rapida"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8dc5fb",
   "metadata": {},
   "source": [
    "## 1. Introduzione"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ec23af",
   "metadata": {},
   "source": [
    "L’esecuzione di pytest senza menzionare il nome dei file, eseguirà tutti i file in formato `test_***.py` o `***_test.py` nella directory o nelle sottodirectory correnti.  Però possiamo fare in modo anche di fare eseguire un altro file, menzionandolo nei comandi. \n",
    "Pytest richiede anche che i nomi delle funzioni di test inzioni con `test`. Nomi di funzioni che non sono di formato test*non sono considerate. Non è possibile far considerare a pytest qualsiasi funzione che non inizi con test come funzione di test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9160306b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def test_sqrt():\n",
    "   num = 25\n",
    "   assert math.sqrt(num) == 5\n",
    "def testsquare():\n",
    "   num = 7\n",
    "   assert 7*7 == 40\n",
    "def tesequality():\n",
    "   assert 10 == 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917aca7f",
   "metadata": {},
   "source": [
    "Eseguendo il comando pytest, genera un output del genere: \n",
    "\n",
    "```python\n",
    "test_square.py .F\n",
    "============================================== FAILURES ==============================================\n",
    "______________________________________________ testsquare _____________________________________________\n",
    "   def testsquare():\n",
    "   num=7\n",
    ">  assert 7*7 == 40\n",
    "E  assert (7 * 7) == 40\n",
    "test_square.py:9: AssertionError\n",
    "================================= 1 failed, 1 passed in 0.06 seconds =================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a12b0d",
   "metadata": {},
   "source": [
    "1)\tLa prima riga, visualizza il nome del file e i risultati. `F` rappresenta un fallimento e il punto (.) rappresenta un successo. Quindi qui, la seconda funzione è fallita.\n",
    "2)\tSuccessivamente si possono vedere i dettagli dei livelli falliti, mostrando quale argomento è fallito. IN questo caso 7x7 viene confrontato con 40, che p sbagliato, non supera il `assert 7*7 == 40` e quindi fallisce\n",
    "3)\tIn fondo ci dice che una funzione è fallita e una è andata a buon fine, senza considerare la terza `tesequality`, questo perché nel nome non c’è la parola test (ma solo `tes`)\n",
    "\n",
    "Inoltre possiamo eseguire il comando con `pytest -v` per aumentare la verbosità, su quale funzione è passata e quale fallita:\n",
    "\n",
    "```python\n",
    "test_square.py::test_sqrt PASSED\n",
    "test_square.py::testsquare FAILED\n",
    "============================================== FAILURES =============================================\n",
    "_____________________________________________ testsquare ____________________________________________\n",
    "   def testsquare():\n",
    "   num = 7\n",
    ">  assert 7*7 == 40\n",
    "E  assert (7 * 7) == 40\n",
    "test_square.py:9: AssertionError\n",
    "================================= 1 failed, 1 passed in 0.04 seconds=================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b91318e",
   "metadata": {},
   "source": [
    "## 2. Esecuzione di un sottoinsieme di test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc8babb",
   "metadata": {},
   "source": [
    "È possibile avere più file di test, per esempio se abbiamo un altro file test_compare.py:\n",
    "```python\n",
    "def test_greater():\n",
    "   num = 100\n",
    "   assert num > 100\n",
    "\n",
    "def test_greater_equal():\n",
    "   num = 100\n",
    "   assert num >= 100\n",
    "\n",
    "def test_less():\n",
    "   num = 100\n",
    "   assert num < 200\n",
    "```\n",
    "Eseguendo il comando pytest -v produrrà questo output, testando insieme test_square.py e test_compare.py: \n",
    "\n",
    "```python\n",
    "test_compare.py::test_greater FAILED\n",
    "test_compare.py::test_greater_equal PASSED\n",
    "test_compare.py::test_less PASSED\n",
    "test_square.py::test_sqrt PASSED\n",
    "test_square.py::testsquare FAILED\n",
    "================================================ FAILURES ================================================\n",
    "______________________________________________ test_greater ______________________________________________\n",
    "   def test_greater():\n",
    "   num = 100\n",
    ">  assert num > 100\n",
    "E  assert 100 > 100\n",
    "\n",
    "test_compare.py:3: AssertionError\n",
    "_______________________________________________ testsquare _______________________________________________\n",
    "   def testsquare():\n",
    "   num = 7\n",
    ">  assert 7*7 == 40\n",
    "E  assert (7 * 7) == 40\n",
    "\n",
    "test_square.py:9: AssertionError\n",
    "=================================== 2 failed, 3 passed in 0.07 seconds ===================================\n",
    "```\n",
    "\n",
    "È possibile eseguire un file specifico con il comando ```pytest test_compare.py -v```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c888ae35",
   "metadata": {},
   "source": [
    "Con pytest è possibile eseguire solo alcuni test, per esempio selezionare solo alcune funzioni. Pytest offre due modi per eseguire il sottoinsieme della suite di test.\n",
    "-\tSelezionare i test da eseguire in base alla corrispondenza della sottostringa dei nomi dei test.\n",
    "-\tSeleziona i gruppi di test da eseguire in base ai marcatori applicati\n",
    "\n",
    "\n",
    "Per eseguire questo tipo di test si puo utilizzare il `-k <substring>` che rappresenta la sottostringa che si vuole testare, es. `pytest -k great -v` che  eseguirà tutti i nomi di test contenenti la parola ‘great’nel suo nome, producendo un output tipo:\n",
    "\n",
    "```python\n",
    "test_compare.py::test_greater FAILED\n",
    "test_compare.py::test_greater_equal PASSED\n",
    "============================================== FAILURES ==============================================\n",
    "____________________________________________ test_greater ____________________________________________\n",
    "def test_greater():\n",
    "num = 100\n",
    ">  assert num > 100\n",
    "E  assert 100 > 100\n",
    "test_compare.py:3: AssertionError\n",
    "========================== 1 failed, 1 passed, 3 deselected in 0.07 seconds ==========================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2db255c",
   "metadata": {},
   "source": [
    "## 3. Raggruppamento test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1b2f0c",
   "metadata": {},
   "source": [
    "Utilizzando i marcatori è possibile raggruppare i test. Pytest consente di utilizzare i marker sulle funzioni di test, engono utilizzati per impostare varie caratteristiche / attributi per testare le funzioni.\n",
    "\n",
    "Pytest fornisce molti marcatori integrati, ma permette anche di creare i propri nomi di marker. I marcatori vengono applicati sui test utilizzando la sintassi: `@pytest.mark.<markername>` mentre per esegurili si utilizza il comando `pytest -m <markername> -v`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fd9153",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_compare.py\n",
    "import pytest\n",
    "\n",
    "@pytest.mark.great\n",
    "def test_greater():\n",
    "   num = 100\n",
    "   assert num > 100\n",
    "\n",
    "@pytest.mark.great\n",
    "def test_greater_equal():\n",
    "   num = 100\n",
    "   assert num >= 100\n",
    "\n",
    "@pytest.mark.others\n",
    "def test_less():\n",
    "   num = 100\n",
    "   assert num < 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f4fd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_square.py\n",
    "import pytest\n",
    "import math\n",
    "\n",
    "@pytest.mark.square\n",
    "def test_sqrt():\n",
    "   num = 25\n",
    "   assert math.sqrt(num) == 5\n",
    "\n",
    "@pytest.mark.square\n",
    "def testsquare():\n",
    "   num = 7\n",
    "   assert 7*7 == 40\n",
    "\n",
    "@pytest.mark.others\n",
    "def test_equality():\n",
    "   assert 10 == 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b5831f",
   "metadata": {},
   "source": [
    "Per eseguire i marker `other` si utilizza il comando `pytest -m others -v`, il che testerà solo due funzioni, con un output: \n",
    "\n",
    "\n",
    "```python\n",
    "test_compare.py::test_less PASSED\n",
    "test_square.py::test_equality FAILED\n",
    "============================================== FAILURES ==============================================\n",
    "___________________________________________ test_equality ____________________________________________\n",
    "   @pytest.mark.others\n",
    "   def test_equality():\n",
    ">  assert 10 == 11\n",
    "E  assert 10 == 11\n",
    "test_square.py:16: AssertionError\n",
    "========================== 1 failed, 1 passed, 4 deselected in 0.08 seconds =========================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d867b27d",
   "metadata": {},
   "source": [
    "## 4. Infissi (fixture)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f42348",
   "metadata": {},
   "source": [
    "Fixture sono funzioni che vengono eseguite prima di ciascunaltra funzione di test. Vengono utilizzati per fornire alcuni dati ai test come connessioni al database, URL da testare e una sorta di dati di input.\n",
    "\n",
    "`@pytest.fixture`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a3c36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_div_by_3_6.py\n",
    "\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def input_value():\n",
    "   input = 39\n",
    "   return input\n",
    "\n",
    "def test_divisible_by_3(input_value):\n",
    "   assert input_value % 3 == 0\n",
    "\n",
    "def test_divisible_by_6(input_value):\n",
    "   assert input_value % 6 == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab493c9d",
   "metadata": {},
   "source": [
    "In questo codice c'è una funzione input_value che fornisce l'input alle due funzioni di test. Durante l'esecuzione del test, Pytest vedrà il nome del dispositivo come parametro di input. Quindi esegue la funzione dispositivo e il valore restituito viene memorizzato nel parametro di input, che può essere utilizzato dal test.\n",
    "\n",
    "Usando il comando `pytest -k divisible -v` verrà generato un output: \n",
    "\n",
    "```python\n",
    "test_div_by_3_6.py::test_divisible_by_3 PASSED\n",
    "test_div_by_3_6.py::test_divisible_by_6 FAILED\n",
    "============================================== FAILURES ==============================================\n",
    "________________________________________ test_divisible_by_6 _________________________________________\n",
    "input_value = 39\n",
    "   def test_divisible_by_6(input_value):\n",
    ">  assert input_value % 6 == 0\n",
    "E  assert (39 % 6) == 0\n",
    "test_div_by_3_6.py:12: AssertionError\n",
    "========================== 1 failed, 1 passed, 6 deselected in 0.07 seconds =========================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c434f7a9",
   "metadata": {},
   "source": [
    "Tuttavia, l'approccio ha i suoi limiti. Una funzione dispositivo definita all'interno di un file di test ha un ambito solo all'interno del file di test. Non possiamo usare quel dispositivo in un altro file di test. Per rendere disponibile una fixture a più file di test, dobbiamo definire la funzione fixture in un file chiamato conftest.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74376d19",
   "metadata": {},
   "source": [
    "## 5. Conftest.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e856bf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conftest.py\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def input_value():\n",
    "   input = 39\n",
    "   return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415eb989",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_div_by_3_6.py\n",
    "\n",
    "import pytest\n",
    "def test_divisible_by_3(input_value):\n",
    "   assert input_value % 3 == 0\n",
    "\n",
    "def test_divisible_by_6(input_value):\n",
    "   assert input_value % 6 == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a49d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_div_by_13.py\n",
    "\n",
    "import pytest\n",
    "def test_divisible_by_13(input_value):\n",
    "   assert input_value % 13 == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd3cda2",
   "metadata": {},
   "source": [
    "Ora avendo i file .py  test_div_by_3_6 e test_div_by_13, utilizzando il comando `pytest -k divisible -v`, Pytest andra a prendere la funzione `input_value` da conftest.py.\n",
    "\n",
    "Il comando genererà un output: \n",
    "\n",
    "```python\n",
    "test_div_by_13.py::test_divisible_by_13 PASSED\n",
    "test_div_by_3_6.py::test_divisible_by_3 PASSED\n",
    "test_div_by_3_6.py::test_divisible_by_6 FAILED\n",
    "============================================== FAILURES ==============================================\n",
    "________________________________________ test_divisible_by_6 _________________________________________\n",
    "input_value = 39\n",
    "   def test_divisible_by_6(input_value):\n",
    ">  assert input_value % 6 == 0\n",
    "E  assert (39 % 6) == 0\n",
    "test_div_by_3_6.py:7: AssertionError\n",
    "========================== 1 failed, 2 passed, 6 deselected in 0.09 seconds ==========================\n",
    "```\n",
    "\n",
    "Inizialmente pytest cercerà il dispositivo nello stesso file, ma poiché non è presenta, controllerà in conftest.py. Quando lo trova in quest'ultimo file viene richiamato il metodo fixture e il risultato viene restituito all'argomento di input del test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f5c16c",
   "metadata": {},
   "source": [
    "## 6. Test di parametrizzazione"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899d37db",
   "metadata": {},
   "source": [
    "La parametrizzazione di un test viene eseguita per eseguire il test su più set di input, è possibile farla utilizzando li codice `@pytest.mark.parametrize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47237709",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_multiplication.py\n",
    "\n",
    "import pytest\n",
    "\n",
    "@pytest.mark.parametrize(\"num, output\",[(1,11),(2,22),(3,35),(4,44)])\n",
    "def test_multiplication_11(num, output):\n",
    "    assert 11*num == output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dc8ed8",
   "metadata": {},
   "source": [
    "Qui il test moltiplica un input per 11 e confronta il risultato con l'output atteso. Il test ha 4 serie di input, ciascuno ha 2 valori: uno è il numero da moltiplicare per 11 e l'altro è il risultato atteso.\n",
    "\n",
    "Utilizzano il comando `Pytest -k multiplication -v` si otterà l'input: \n",
    "\n",
    "```python\n",
    "test_multiplication.py::test_multiplication_11[1-11] PASSED\n",
    "test_multiplication.py::test_multiplication_11[2-22] PASSED\n",
    "test_multiplication.py::test_multiplication_11[3-35] FAILED\n",
    "test_multiplication.py::test_multiplication_11[4-44] PASSED\n",
    "============================================== FAILURES ==============================================\n",
    "_________________ test_multiplication_11[3-35] __________________\n",
    "num = 3, output = 35\n",
    "   @pytest.mark.parametrize(\"num, output\",[(1,11),(2,22),(3,35),(4,44)])\n",
    "   def test_multiplication_11(num, output):\n",
    ">  assert 11*num == output\n",
    "E  assert (11 * 3) == 35\n",
    "test_multiplication.py:5: AssertionError\n",
    "============================== 1 failed, 3 passed, 8 deselected in 0.08 seconds ==============================\n",
    "```\n",
    "\n",
    "Il questo caso il test fallisce sul terzo input, in quanto 11*3 non è uguale a 35 ma bensi 33."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218d4221",
   "metadata": {},
   "source": [
    "## 7. Xfail - Salta test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5b857d",
   "metadata": {},
   "source": [
    "Può capitare che un test non sia rilevante per una determinata analisi, perchè per esempio si sta analizzando altro. Quindi con xfail c'è la possibilità di far fallire quel determinato test e di non considerarlo, con il codice `@pytest.mark.xfail`\n",
    "\n",
    "Oltre a far fallire il test, c'è la possibilità di saltarlo, ovvero che il test non verrà eseguito. Con il codice `@pytest.mark.skip`\n",
    "\n",
    "Pytest in ognicaso eseguirà il test xfailed, ma non sarà considerato come test parzialmente fallito o superato. I dettagli di questi test non verranno stampati anche se il test fallisce.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9285cf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_compare.py\n",
    "\n",
    "import pytest\n",
    "\n",
    "@pytest.mark.xfail\n",
    "@pytest.mark.great\n",
    "def test_greater():\n",
    "   num = 100\n",
    "   assert num > 100\n",
    "\n",
    "@pytest.mark.xfail\n",
    "@pytest.mark.great\n",
    "def test_greater_equal():\n",
    "   num = 100\n",
    "   assert num >= 100\n",
    "\n",
    "@pytest.mark.skip\n",
    "@pytest.mark.others\n",
    "def test_less():\n",
    "   num = 100\n",
    "   assert num < 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dba47bd",
   "metadata": {},
   "source": [
    "Eseguiendo il test su quest tre funzioni, pytest generà un output così: \n",
    "\n",
    "```python\n",
    "test_compare.py::test_greater xfail\n",
    "test_compare.py::test_greater_equal XPASS\n",
    "test_compare.py::test_less SKIPPED\n",
    "============================ 1 skipped, 1 xfailed, 1 xpassed in 0.06 seconds ============================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a49035",
   "metadata": {},
   "source": [
    "## 8. Arresta Test Suite dopo N test falliti"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370916f7",
   "metadata": {},
   "source": [
    "In uno scenario reale, una volta che una nuova versione del codice è pronta per la distribuzione, viene prima distribuita nell'ambiente di pre-produzione / gestione temporanea per essere testato. Quindi viene eseguita una suite di test.\n",
    "\n",
    "Il codice è pronto per la distribuzione in produzione solo se la suite di test ha esito positivo. Se si verifica un errore del test, che sia uno o più, il codice non è pronto per la produzione.\n",
    "\n",
    "Quindi, cosa succederebbe se volessimo interrompere l'esecuzione della suite di test subito dopo n numero di test falliti. Questo può essere fatto in pytest usando maxfail.\n",
    "\n",
    "Il comando per interrompere l'esecuzione della suite di test subito dopo n numero di test falliti è: `pytest --maxfail = <num>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde86387",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_failure.py\n",
    "\n",
    "import pytest\n",
    "import math\n",
    "\n",
    "def test_sqrt_failure():\n",
    "   num = 25\n",
    "   assert math.sqrt(num) == 6\n",
    "\n",
    "def test_square_failure():\n",
    "   num = 7\n",
    "   assert 7*7 == 40\n",
    "\n",
    "def test_equality_failure():\n",
    "   assert 10 == 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a5c788",
   "metadata": {},
   "source": [
    "`pytest test_failure.py -v --maxfail = 1` -> in questo caso tutti i 3 test falliranno, ma verrà interrotta l'esecuzione dopo un errore.\n",
    "\n",
    "```python\n",
    "test_failure.py::test_sqrt_failure FAILED\n",
    "=================================== FAILURES =================================== \n",
    "_______________________________________ test_sqrt_failure __________________________________________\n",
    "   def test_sqrt_failure():\n",
    "   num = 25\n",
    ">  assert math.sqrt(num) == 6\n",
    "E  assert 5.0 == 6\n",
    "E  + where 5.0 = <built-in function sqrt>(25)\n",
    "E  + where <built-in function sqrt>= math.sqrt\n",
    "test_failure.py:6: AssertionError\n",
    "=============================== 1 failed in 0.04 seconds ==============================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6af93f",
   "metadata": {},
   "source": [
    "## 9. Test in parallelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0cb752",
   "metadata": {},
   "source": [
    "Pytest di base esegue i test in ordine sequenziale, quando si hanno tanti test questo porta ad un lungo tempo di esecuzione.  Per ovviare a questo problema, pytest permette di eseguire i test in parallelo.\n",
    "\n",
    "Per fare questo bisogna installare un plugin di pytest `pip install pytest-xdist`, e per usare questo plugin basta eseguire il comando `pytest -n 3`.\n",
    "In questo caso pytest eseguirà 3 test in contemporanea. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2779c134",
   "metadata": {},
   "source": [
    "## 10. Risultati in fomrato XML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed266dbe",
   "metadata": {},
   "source": [
    "Pytest offre la possibilità di generare i risultati dell'esecuzione del test in un file `.xml`.  Questo file xml è utile principalmente nei casi in cui abbiamo una dashboard che proietta i risultati del test.\n",
    "\n",
    "Per salvare questi dati in xml si utilizza il comando `--junitxml=\"result.xml\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9327db6d",
   "metadata": {},
   "source": [
    "Eseguendo il comando `pytest test_multiplication.py -v --junitxml=\"result.xml\"` per il file test_multiplication.py si avrà un file xml così:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd667d9f",
   "metadata": {
    "vscode": {
     "languageId": "xml"
    }
   },
   "outputs": [],
   "source": [
    "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
    "<testsuites name=\"pytest tests\">\n",
    "    <testsuite name=\"pytest\" errors=\"0\" failures=\"1\" skipped=\"0\" tests=\"4\" time=\"0.775\" timestamp=\"2025-11-11T09:54:55.118060+01:00\" hostname=\"DESKCNA398\">\n",
    "        <testcase classname=\"test_multiplication\" name=\"test_multiplication_11[1-11]\" time=\"0.003\" />\n",
    "        <testcase classname=\"test_multiplication\" name=\"test_multiplication_11[2-22]\" time=\"0.001\" />\n",
    "        <testcase classname=\"test_multiplication\" name=\"test_multiplication_11[3-35]\" time=\"0.002\">\n",
    "            <failure message=\"assert (11 * 3) == 35\">\n",
    "                num = 3, output = 35\n",
    "\n",
    "                @pytest.mark.parametrize(\"num, output\",[(1,11),(2,22),(3,35),(4,44)])\n",
    "                def test_multiplication_11(num, output):\n",
    "            &gt;    assert 11*num == output\n",
    "            E       assert (11 * 3) == 35\n",
    "\n",
    "                test_multiplication.py:7: AssertionError\n",
    "            </failure>\n",
    "        </testcase>\n",
    "        <testcase classname=\"test_multiplication\" name=\"test_multiplication_11[4-44]\" time=\"0.001\" />\n",
    "    </testsuite>\n",
    "</testsuites>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a583edf7",
   "metadata": {},
   "source": [
    "Qui, il tag `<testsuit>` riassume che ci sono stati 4 test e il numero di fallimenti è 1.\n",
    "\n",
    "- Il tag `<testcase>` fornisce i dettagli di ogni test eseguito.\n",
    "\n",
    "- Il tag `<failure>` fornisce i dettagli del codice di test fallito."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
